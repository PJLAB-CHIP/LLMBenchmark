Benchmark:
  CUDA_VISIBLE_DEVICES: "0,1"
  device: &_device "cuda"  
  model: &_model "meta-llama/llama-2-7b-hf"
  hardware_name: "A100-SXM4-80GB"
  result_path: "./results/llama-2-7b.csv"
  n_warm_up: 1
  TestSet:
    - prompt_size: [512]
      batch_size: [1]
      token_size: [256, 1024, 2048, 4096, 8192]
    - prompt_size: [512]
      batch_size: [1,2,4,8,16,32,64]
      token_size: [128]
    - prompt_size: [128, 256, 1024, 2048, 4096, 8192]
      batch_size: [1]
      token_size: [128]

Model:
  Options:
    load_weights: true
    device: *_device
  Params:
    pretrained_model_name_or_path: *_model
    cache_dir: "/root/model_hub/"
    torch_dtype: "float16"
    device_map: "auto"
    trust_remote_code: true
    attn_implementation: "flash_attention_2"

Tokenizer:
  Params:
    pretrained_model_name_or_path: *_model
    cache_dir: "/root/model_hub/"
    trust_remote_code: true